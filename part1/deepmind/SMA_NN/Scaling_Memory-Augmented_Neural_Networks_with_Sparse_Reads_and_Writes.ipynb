{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 싸이그래머 / ReMind : 파트 1 - 딥마인드 논문 리뷰 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* Abstract\n",
    "* 1 Introduction\n",
    "* 2 Background\n",
    "    - 2.1 Attention and content-based addressing\n",
    "    - 2.2 Memory Networks\n",
    "    - 2.3 Neural Turing Machine\n",
    "* 3 Architecture\n",
    "    - 3.1 Read\n",
    "    - 3.2 Write\n",
    "    - 3.3 Controller\n",
    "    - 3.4 Efficient backpropagation through time\n",
    "    - 3.5 Approximate nearest neighbors\n",
    "* 4 Results\n",
    "    - 4.1 Speed and memory benchmarks\n",
    "    - 4.2 Learning with sparse memory access\n",
    "    - 4.3 Scaling with a curriculum\n",
    "    - 4.4 Question answering on the Babi tasks\n",
    "    - 4.5 Learning on real world data\n",
    "* 5 Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [1] Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes - https://arxiv.org/abs/1610.09027\n",
    "* [2] Daniel Shank, Data Scientist, Talla at MLconf SF 2016 - http://www.slideshare.net/SessionsEvents/daniel-shank-data-scientist-talla-at-mlconf-sf-2016\n",
    "* [3] [DL輪読会] Hybrid computing using a neural network with dynamic external memory -  http://www.slideshare.net/YuusukeIwasawa/dl-hybrid-computing-using-a-neural-network-with-dynamic-external-memory\n",
    "* [4] DNC review -  https://github.com/psygrammer/dgm/blob/master/part1/deepmind/DNC/Hybrid_computing_using_a_neural_network_with_dynamic_external_memory.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. \n",
    "    - These models appear promising for applications such as language modeling and machine translation. \n",
    "* <font color=\"blue\">However, they scale poorly in both space and time as the amount of memory grows — limiting their applicability to real-world domains</font>. \n",
    "* <font color=\"red\">Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM)</font>, \n",
    "    - that retains the representational power of the original approaches whilst training efficiently with very large memories. \n",
    "    - We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs <font color=\"blue\">1,000× faster and with 3,000× less physical memory than non-sparse models</font>. \n",
    "    - SAM learns with comparable \n",
    "        - data efficiency to existing models on \n",
    "            - a range of synthetic tasks and \n",
    "            - one-shot Omniglot character recognition, and \n",
    "        - can scale to tasks requiring 100,000s of time steps and memories. \n",
    "* As well, we show how our approach can be \n",
    "    - <font color=\"red\">adapted</font> for models that maintain temporal associations between memories, \n",
    "        - as with the recently introduced <font color=\"red\">Differentiable Neural Computer</font>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A significant difficulty in training these models results from their smooth read and write operations, which incur linear computational overhead on the number of memories stored per time step of training. Even worse, they require duplication of the entire memory at each time step to perform backpropagation through time (BPTT). To deal with sufficiently complex problems, such as processing book, or Wikipedia, this overhead becomes prohibitive. For example, to store 64 memories, a straightforward implementation of the NTM trained over a sequence of length 100 consumes ≈ 30 MiB physical memory; to store 64,000 memories the overhead exceeds 29 GiB (see Figure 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.png\", width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Further, in Supplementary D we demonstrate the generality of our approach by describing how to construct a sparse version of the recently published Differentiable Neural Computer [8]. This Sparse Differentiable Neural Computer (SDNC) is over 400× faster than the canonical dense variant for a memory size of 2,000 slots, and achieves the best reported result in the Babi tasks without supervising the memory access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Background\n",
    "* 2.1 Attention and content-based addressing\n",
    "* 2.2 Memory Networks\n",
    "* 2.3 Neural Turing Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Attention and content-based addressing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\", width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Memory Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [5] Neural networks with external memory - https://courses.cs.ut.ee/MTAT.03.292/2016_fall/uploads/Main/externalmemory.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Neural Turing Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [6] Neural Turing Machines - http://www.slideshare.net/iljakuzovkin/neural-turing-machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural Turing Machine is a recurrent neural network equipped with a content-addressable memory, similar to Memory Networks, but with the additional capability to write to memory over time. The memory is accessed by a controller network, typically an LSTM, and the full model is differentiable — allowing it to be trained via BPTT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A write to memory,\n",
    "<img src=\"figures/cap2.png\", width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Architecture\n",
    "* 3.1 Read\n",
    "* 3.2 Write\n",
    "* 3.3 Controller\n",
    "* 3.4 Efficient backpropagation through time\n",
    "* 3.5 Approximate nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper introduces Sparse Access Memory (SAM), a new neural memory architecture with two innovations. \n",
    "* Most importantly, \n",
    "    - all writes to and reads from external memory are \n",
    "        - constrained to a sparse subset of the memory words, \n",
    "            - providing similar functionality as the NTM, \n",
    "            - while allowing computational and memory efficient operation. \n",
    "* Secondly, \n",
    "    - we introduce a sparse memory management scheme that \n",
    "        - tracks memory usage and \n",
    "        - finds unused blocks of memory \n",
    "            - for recording new information.\n",
    "\n",
    "For a memory containing N words, \n",
    "* SAM executes a forward, backward step in Θ(log N ) time, \n",
    "* initializes in Θ(N ) space, and \n",
    "* consumes Θ(1) space per time step. \n",
    "* Under some reasonable assumptions, \n",
    "    - SAM is asymptotically optimal in time and space complexity (Supplementary A)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse read operation is defined to be a weighted average over a selection of words in memory:\n",
    "<img src=\"figures/cap3.png\", width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O(N) time -> we can use an approximate nearest neighbor data-structure, described in Section 3.5, -> O(log N ) time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse read can be considered a special case of the matrix-vector product defined in (1), with two key distinctions. \n",
    "* The first is that \n",
    "    - we pass gradients for \n",
    "        - only a constant K number of rows \n",
    "            - of memory per time step, versus N, \n",
    "        - which results in a negligible fraction of non-zero error gradient per timestep \n",
    "            - when the memory is large. \n",
    "* The second distinction is in implementation: \n",
    "    - by <font color=\"red\">using an efficient sparse matrix format</font> such as <font color=\"blue\">Compressed Sparse Rows (CSR)</font>, \n",
    "    - we can compute (4) and its gradients in constant time and space (see Supplementary A)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The write operation is SAM is an instance of (3) where the write weights w ̃tW are constrained to contain a constant number of non-zero entries. \n",
    "* This is done by a simple scheme where the controller writes either to previously read locations, in order to <font color=\"red\">update contextually relevant memories</font>, or <font color=\"red\">the least recently accessed location</font>, in order to overwrite stale or <font color=\"red\">unused memory slots with fresh content</font>.\n",
    "* The introduction of sparsity could be achieved via other write schemes. \n",
    "    - For example, we could use a sparse content-based write scheme, where the controller chooses a query vector qtW and applies writes to similar words in memory. This would allow for direct memory updates, but would create problems when the memory is empty (and shift further complexity to the controller). \n",
    "* We decided upon the <font color=\"red\">previously read / least recently accessed addressing scheme</font> <font color=\"blue\">for simplicity and flexibility</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The write weights are defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap4.png\", width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap5.png\", width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a one layer LSTM for the controller throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM then produces a vector, $p_t$ = ($q_t$, $a_t$, $α_t$, $γ_t$), of read and write parameters for memory access via a linear layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/6fig.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Efficient backpropagation through time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig5.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Approximate nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When querying the memory, \n",
    "* we can use an approximate nearest neighbor index (ANN) to search over the external memory for the K nearest words. \n",
    "* Where a linear KNN search inspects every element in memory (taking O(N) time), an ANN index maintains a structure over the dataset to allow for fast inspection of nearby points in O(log N ) time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We considered two types of ANN indexes: \n",
    "* FLANN’s randomized k-d tree implementation that \n",
    "    - arranges the datapoints in an ensemble of structured (randomized k-d) trees to search for nearby points via comparison-based search, and \n",
    "* one that uses locality sensitive hash (LSH) functions that \n",
    "    - map points into buckets with distance-preserving guarantees. \n",
    "    \n",
    "We used randomized k-d trees for small word sizes and LSHs for large word sizes. \n",
    "* For both ANN implementations, there is an O(log N ) cost for insertion, deletion and query.\n",
    "\n",
    "We also rebuild the ANN from scratch every N insertions to ensure it does not become imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Results\n",
    "* 4.1 Speed and memory benchmarks\n",
    "* 4.2 Learning with sparse memory access\n",
    "* 4.3 Scaling with a curriculum\n",
    "* 4.4 Question answering on the Babi tasks\n",
    "* 4.5 Learning on real world data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Speed and memory benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.png\", width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Learning with sparse memory access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.png\", width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Scaling with a curriculum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.png\", width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig8.png\", width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Question answering on the Babi tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [7] Playground for bAbI tasks - https://yerevann.github.io/2016/02/23/playground-for-babi-tasks/\n",
    "* [8] Dynamic Memory Networks by YerevanNN Web Demo - http://yerevann.com/dmn-ui/#/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/tab1.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Learning on real world data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [9] brendenlake/omniglot - https://github.com/brendenlake/omniglot\n",
    "* [10] One-Shot Learning - http://www.slideshare.net/JisungDavidKim/oneshot-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.png\", width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes - https://arxiv.org/abs/1610.09027\n",
    "* [2] Daniel Shank, Data Scientist, Talla at MLconf SF 2016 - http://www.slideshare.net/SessionsEvents/daniel-shank-data-scientist-talla-at-mlconf-sf-2016\n",
    "* [3] [DL輪読会] Hybrid computing using a neural network with dynamic external memory -  http://www.slideshare.net/YuusukeIwasawa/dl-hybrid-computing-using-a-neural-network-with-dynamic-external-memory\n",
    "* [4] DNC review -  https://github.com/psygrammer/dgm/blob/master/part1/deepmind/DNC/Hybrid_computing_using_a_neural_network_with_dynamic_external_memory.ipynb\n",
    "* [5] Neural networks with external memory - https://courses.cs.ut.ee/MTAT.03.292/2016_fall/uploads/Main/externalmemory.pdf\n",
    "* [6] Neural Turing Machines - http://www.slideshare.net/iljakuzovkin/neural-turing-machines\n",
    "* [7] Playground for bAbI tasks - https://yerevann.github.io/2016/02/23/playground-for-babi-tasks/\n",
    "* [8] Dynamic Memory Networks by YerevanNN Web Demo - http://yerevann.com/dmn-ui/#/\n",
    "* [9] brendenlake/omniglot - https://github.com/brendenlake/omniglot\n",
    "* [10] One-Shot Learning - http://www.slideshare.net/JisungDavidKim/oneshot-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
